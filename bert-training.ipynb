{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport gc\nimport math\nimport random\nimport time\nimport datetime\nfrom tqdm import tqdm\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.neighbors import NearestNeighbors\n\nimport torch\nfrom torch import nn \nimport torch.nn.functional as F \nfrom transformers import AutoTokenizer, AutoModel\nfrom torch.optim.lr_scheduler import _LRScheduler\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom transformers import AdamW\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nTRAIN_CSV = '../input/shopee-product-matching/train.csv'\nrandom.seed(512)","metadata":{"execution":{"iopub.status.busy":"2022-06-09T14:18:44.749883Z","iopub.execute_input":"2022-06-09T14:18:44.750166Z","iopub.status.idle":"2022-06-09T14:18:44.757517Z","shell.execute_reply.started":"2022-06-09T14:18:44.750136Z","shell.execute_reply":"2022-06-09T14:18:44.756508Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"### Configuration","metadata":{}},{"cell_type":"code","source":"bert_model_name = '../input/transformers/bert-base-uncased'\n\nmax_length = 128\n\nscale = 30\nmargin = 0.5\nfc_dim = 768\nseed = 412\nclasses = 11014\n\nn_splits = 4 \nbatch_size = 16\naccum_iter = 1\nepochs = 12\nmin_save_epoch = epochs // 3\nnum_workers = 2\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n\nbert_knn = 50\nbert_knn_threshold = 0.4\nscheduler = _LRScheduler\n\nscheduler_params = {\n    \"lr_start\": 7.5e-6,\n    \"lr_max\": 1e-4,\n    \"lr_min\": 2.74e-5\n}\nmultiplier = scheduler_params['lr_max'] / scheduler_params['lr_start']\neta_min = scheduler_params['lr_min']\nfreeze_epo = 0\nwarmup_epo = 2\ncosine_epo = epochs - freeze_epo - warmup_epo\n\nsave_model_path = f\"./{bert_model_name.rsplit('/', 1)[-1]}_epoch{epochs}-bs{batch_size}x{accum_iter}.pt\"","metadata":{"execution":{"iopub.status.busy":"2022-06-09T14:18:44.848531Z","iopub.execute_input":"2022-06-09T14:18:44.849309Z","iopub.status.idle":"2022-06-09T14:18:44.857215Z","shell.execute_reply.started":"2022-06-09T14:18:44.849237Z","shell.execute_reply":"2022-06-09T14:18:44.856544Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"### Dataset","metadata":{}},{"cell_type":"code","source":"class TitleDataset(torch.utils.data.Dataset):\n    def __init__(self, df, text_column, label_column):\n        texts = df[text_column]\n        self.labels = df[label_column].values\n        self.titles = []\n        for title in texts:\n            title = title.encode('utf-8').decode(\"unicode_escape\")\n            title = title.encode('ascii', 'ignore').decode(\"unicode_escape\")\n            title = title.lower()\n            self.titles.append(title)\n\n    def __len__(self):\n        return len(self.titles)\n\n    def __getitem__(self, idx):\n        text = self.titles[idx]\n        label = torch.tensor(self.labels[idx])\n        return text, label","metadata":{"execution":{"iopub.status.busy":"2022-06-09T14:18:45.022220Z","iopub.execute_input":"2022-06-09T14:18:45.022685Z","iopub.status.idle":"2022-06-09T14:18:45.031382Z","shell.execute_reply.started":"2022-06-09T14:18:45.022654Z","shell.execute_reply":"2022-06-09T14:18:45.030674Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"### Training function and metrics","metadata":{}},{"cell_type":"code","source":"def train_fn(model, data_loader, optimizer, scheduler, accum_iter, epoch, device):\n    model.train()\n    fin_loss = 0.0\n    tk = tqdm(data_loader, desc = \"Training epoch: \" + str(epoch+1), ncols=100)\n\n    for t, (texts, labels) in enumerate(tk):\n        texts = list(texts)\n\n        _, loss = model(texts, labels)\n        loss.backward()\n        fin_loss += loss.item() \n\n        if (t + 1) % accum_iter == 0:\n            optimizer.step() \n            optimizer.zero_grad()\n                \n        tk.set_postfix({'loss' : '%.6f' %float(fin_loss/(t+1)), 'LR' : optimizer.param_groups[0]['lr']})\n\n    return model, fin_loss / len(data_loader)","metadata":{"execution":{"iopub.status.busy":"2022-06-09T14:18:45.077297Z","iopub.execute_input":"2022-06-09T14:18:45.077811Z","iopub.status.idle":"2022-06-09T14:18:45.085123Z","shell.execute_reply.started":"2022-06-09T14:18:45.077782Z","shell.execute_reply":"2022-06-09T14:18:45.084123Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"def getMetric(col):\n    def f1score(row):\n        n = len(np.intersect1d(row.target, row[col]))\n        return 2 * n / (len(row.target) + len(row[col]))\n    return f1score\n\n\ndef get_bert_embeddings(df, column, model, chunk=32):\n    model.eval()\n    \n    bert_embeddings = torch.zeros((df.shape[0], 768)).to(device)\n    for i in tqdm(list(range(0, df.shape[0], chunk)) + [df.shape[0]-chunk], desc=\"get_bert_embeddings\", ncols=80):\n        titles = []\n        for title in df[column][i : i + chunk].values:\n            try:\n                title = title.encode('utf-8').decode(\"unicode_escape\")\n                title = title.encode('ascii', 'ignore').decode(\"unicode_escape\")\n            except:\n                pass\n            title = title.lower()\n            titles.append(title)\n            \n        with torch.no_grad():\n            model_output = model(titles)\n            \n        bert_embeddings[i : i + chunk] = model_output\n    \n    del model, titles, model_output\n    gc.collect()\n    torch.cuda.empty_cache()\n    \n    return bert_embeddings\n\n\ndef get_neighbors(df, embeddings, knn=50, threshold=0.0):\n\n    model = NearestNeighbors(n_neighbors=knn, metric='cosine')\n    model.fit(embeddings)\n    distances, indices = model.kneighbors(embeddings)\n    \n    preds = []\n    for k in range(embeddings.shape[0]):\n        idx = np.where(distances[k,] < threshold)[0]\n        ids = indices[k,idx]\n        posting_ids = df['posting_id'].iloc[ids].values\n        preds.append(posting_ids)\n        \n    del model, distances, indices\n    gc.collect()\n    return preds","metadata":{"execution":{"iopub.status.busy":"2022-06-09T14:18:45.132327Z","iopub.execute_input":"2022-06-09T14:18:45.132781Z","iopub.status.idle":"2022-06-09T14:18:45.144633Z","shell.execute_reply.started":"2022-06-09T14:18:45.132747Z","shell.execute_reply":"2022-06-09T14:18:45.143978Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"class ArcMarginProduct(nn.Module):\n    def __init__(self, in_features, out_features, scale=30.0, margin=0.50, ls_eps=0.0):\n        super(ArcMarginProduct, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.scale = scale\n        self.margin = margin\n        self.ls_eps = ls_eps \n        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))\n        nn.init.xavier_uniform_(self.weight)\n\n        self.cos_m = math.cos(margin)\n        self.sin_m = math.sin(margin)\n        self.th = math.cos(math.pi - margin)\n        self.mm = math.sin(math.pi - margin) * margin\n        \n        self.criterion = nn.CrossEntropyLoss()\n                \n    def forward(self, input, label):\n        cosine = F.linear(F.normalize(input), F.normalize(self.weight))\n        sine = torch.sqrt(1.0 - torch.pow(cosine, 2))\n        phi = cosine * self.cos_m - sine * self.sin_m\n        phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n\n        one_hot = torch.zeros(cosine.size(), device=device)\n        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n        if self.ls_eps > 0:\n            one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps / self.out_features\n\n        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n        output *= self.scale\n        return output, self.criterion(output,label)","metadata":{"execution":{"iopub.status.busy":"2022-06-09T14:18:45.189980Z","iopub.execute_input":"2022-06-09T14:18:45.190767Z","iopub.status.idle":"2022-06-09T14:18:45.201708Z","shell.execute_reply.started":"2022-06-09T14:18:45.190712Z","shell.execute_reply":"2022-06-09T14:18:45.201018Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"### Model","metadata":{}},{"cell_type":"code","source":"class BertModel(nn.Module):\n\n    def __init__(\n        self,\n        n_classes = classes,\n        model_name = bert_model_name,\n        fc_dim = fc_dim,\n        margin = margin,\n        scale = scale,\n        use_fc = True\n    ):\n        super(BertModel,self).__init__()\n        \n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        self.model = AutoModel.from_pretrained(model_name).to(device)\n\n        in_features = 768\n            \n        self.final = ArcMarginProduct(\n            in_features,\n            n_classes,\n            scale = scale,\n            margin = margin,\n            ls_eps = 0.0\n        )\n\n    def forward(self, texts, labels=torch.tensor([0])):\n        encoding = self.tokenizer(texts, padding=True, truncation=True,\n                             max_length=max_length, return_tensors='pt').to(device)\n        input_ids = encoding['input_ids']\n        attention_mask = encoding['attention_mask']\n        embedding = self.model(input_ids, attention_mask=attention_mask)\n        \n        token_embeddings = embedding[0]\n        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n        sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n        sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n        features = sum_embeddings / sum_mask\n        if self.training:\n            logits = self.final(features, labels.to(device))\n            return logits\n        else:\n            return features","metadata":{"execution":{"iopub.status.busy":"2022-06-09T14:18:45.242235Z","iopub.execute_input":"2022-06-09T14:18:45.242859Z","iopub.status.idle":"2022-06-09T14:18:45.252315Z","shell.execute_reply.started":"2022-06-09T14:18:45.242825Z","shell.execute_reply":"2022-06-09T14:18:45.251584Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(TRAIN_CSV)\ndf['target'] = df.label_group.map(df.groupby('label_group').posting_id.agg('unique').to_dict())\n\ngkf = GroupKFold(n_splits=n_splits)\ndf['fold'] = -1\nfor i, (train_idx, valid_idx) in enumerate(gkf.split(X=df, groups=df['label_group'])):\n    df.loc[valid_idx, 'fold'] = i\n\nlabelencoder= LabelEncoder()\ndf['label_group'] = labelencoder.fit_transform(df['label_group'])\n\nfold = 0\ntrain_df = df[df['fold']!=fold].reset_index(drop=True)\nvalid_df = df[df['fold']==fold].reset_index(drop=True)\nprint(\"train_df length =\", len(train_df))\nprint(\"train_df classes =\", len(train_df['label_group'].unique()))\nprint(\"valid_df length =\", len(valid_df))\nprint(\"valid_df classes =\", len(valid_df['label_group'].unique()))\n\ntrain_dataset = TitleDataset(train_df, 'title', 'label_group')\ntrain_dataloader = torch.utils.data.DataLoader(\n    train_dataset,\n    batch_size = batch_size,\n    num_workers = num_workers,\n    pin_memory = True,\n    shuffle = True,\n    drop_last = True\n)\n\nvalid_dataset = TitleDataset(valid_df, 'title', 'label_group')\nvalid_dataloader = torch.utils.data.DataLoader(\n    valid_dataset,\n    batch_size = batch_size,\n    num_workers = num_workers,\n    pin_memory = True,\n    shuffle = False,\n    drop_last = False\n)","metadata":{"execution":{"iopub.status.busy":"2022-06-09T14:18:45.325692Z","iopub.execute_input":"2022-06-09T14:18:45.326328Z","iopub.status.idle":"2022-06-09T14:18:46.131739Z","shell.execute_reply.started":"2022-06-09T14:18:45.326292Z","shell.execute_reply":"2022-06-09T14:18:46.131015Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"model = BertModel()\nmodel.to(device)\n\noptimizer_grouped_parameters = [\n    {'params': model.model.parameters(), 'lr': scheduler_params['lr_start']},\n    {'params': model.final.parameters(), 'lr': scheduler_params['lr_start'] * 2},\n]\noptimizer = AdamW(optimizer_grouped_parameters)","metadata":{"execution":{"iopub.status.busy":"2022-06-09T14:18:46.133244Z","iopub.execute_input":"2022-06-09T14:18:46.134347Z","iopub.status.idle":"2022-06-09T14:18:48.368136Z","shell.execute_reply.started":"2022-06-09T14:18:46.134309Z","shell.execute_reply":"2022-06-09T14:18:48.367218Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"raw","source":"print(\"Training epochs =\", epochs)","metadata":{"execution":{"iopub.status.busy":"2022-06-09T14:18:48.374275Z","iopub.execute_input":"2022-06-09T14:18:48.374638Z","iopub.status.idle":"2022-06-09T14:18:48.383640Z","shell.execute_reply.started":"2022-06-09T14:18:48.374598Z","shell.execute_reply":"2022-06-09T14:18:48.382712Z"}}},{"cell_type":"code","source":"max_f1_valid = 0.\n\nfor epoch in range(epochs):\n    model, avg_loss_train = train_fn(\n        model, train_dataloader, optimizer, scheduler, accum_iter, epoch, device\n    )\n\n    valid_embeddings = get_bert_embeddings(valid_df, 'title', model)\n    valid_predictions = get_neighbors(valid_df, valid_embeddings.detach().cpu().numpy(),\n                                      knn=bert_knn if len(df) > 3 else 3, threshold=bert_knn_threshold)\n\n    valid_df['oof'] = valid_predictions\n    valid_df['f1'] = valid_df.apply(getMetric('oof'), axis=1)\n    valid_f1 = valid_df.f1.mean()\n    print('Valid f1 score =', valid_f1)\n\n    if (epoch >= min_save_epoch) and (valid_f1 > max_f1_valid):\n        print(f\"[{datetime.datetime.now()}] Valid f1 score improved. Saving model weights to {save_model_path}\")\n        max_f1_valid = valid_f1\n        torch.save(model.state_dict(), save_model_path)","metadata":{"execution":{"iopub.status.busy":"2022-06-09T14:18:48.385569Z","iopub.execute_input":"2022-06-09T14:18:48.386412Z","iopub.status.idle":"2022-06-09T14:50:30.669994Z","shell.execute_reply.started":"2022-06-09T14:18:48.386372Z","shell.execute_reply":"2022-06-09T14:50:30.669023Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"print(\"Searching best threshold...\")\n\nsearch_space = np.arange(30, 50, 1)\ntresholds = []\nf1 = []\n\nmodel.load_state_dict(torch.load(save_model_path, map_location=device))\nvalid_embeddings = get_bert_embeddings(valid_df, 'title', model)\n\nbest_f1_valid = 0.\nbest_threshold = 0.\n\nfor i in search_space:\n    threshold = i / 100\n    valid_predictions = get_neighbors(valid_df, valid_embeddings.detach().cpu().numpy(),\n                                      knn=bert_knn if len(df) > 3 else 3, threshold=threshold)\n\n    valid_df['oof'] = valid_predictions\n    valid_df['f1'] = valid_df.apply(getMetric('oof'), axis=1)\n    valid_f1 = valid_df.f1.mean()\n    tresholds.append(threshold)\n    f1.append(valid_f1)\n    print(f\"threshold = {threshold} -> f1 score = {valid_f1}\")\n\n    if (valid_f1 > best_f1_valid):\n        best_f1_valid = valid_f1\n        best_threshold = threshold\n\nprint(\"Best threshold =\", best_threshold)\nprint(\"Best f1 score =\", best_f1_valid)\nBEST_THRESHOLD = best_threshold","metadata":{"execution":{"iopub.status.busy":"2022-06-09T15:09:12.151193Z","iopub.execute_input":"2022-06-09T15:09:12.151902Z","iopub.status.idle":"2022-06-09T15:10:26.122813Z","shell.execute_reply.started":"2022-06-09T15:09:12.151866Z","shell.execute_reply":"2022-06-09T15:10:26.122079Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"import plotly.graph_objects as go\nimport numpy as np\n\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=tresholds, y=f1,\n                    mode='lines+markers',\n                    name='Threshold selection'))\nfig.update_layout(legend=dict(\n    yanchor=\"top\",\n    xanchor=\"right\",\n    y = 0.3,\n    x = 0.99\n))\nfig.update_layout(title_text='Threshold selection', title_x=0.5)\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-09T15:13:07.915377Z","iopub.execute_input":"2022-06-09T15:13:07.916072Z","iopub.status.idle":"2022-06-09T15:13:08.086475Z","shell.execute_reply.started":"2022-06-09T15:13:07.916034Z","shell.execute_reply":"2022-06-09T15:13:08.084614Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}